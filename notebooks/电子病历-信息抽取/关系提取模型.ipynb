{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as tdata\n",
    "from keras import callbacks\n",
    "from keras import optimizers\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from htorch.contrib.data.reader import SKE2019Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = SKE2019Reader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = reader.read(\"/home/heyao/projects/htorch/data/SKE-2019/train_data.json\")\n",
    "dev_data = reader.read(\"/home/heyao/projects/htorch/data/SKE-2019/dev_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>word_segs</th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>subject</th>\n",
       "      <th>subject_type</th>\n",
       "      <th>object</th>\n",
       "      <th>object_type</th>\n",
       "      <th>predicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>内容简介《宜兴紫砂图典》由故宫出版社出版</td>\n",
       "      <td>内容|简介|《|宜兴紫砂图典|》|由|故宫出版社|出版</td>\n",
       "      <td>n|n|w|nw|w|p|nt|v</td>\n",
       "      <td>宜兴紫砂图典</td>\n",
       "      <td>书籍</td>\n",
       "      <td>故宫出版社</td>\n",
       "      <td>出版社</td>\n",
       "      <td>出版社</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>《中国风水十讲》是2007年华夏出版社出版的图书，作者是杨文衡</td>\n",
       "      <td>《|中国风水十讲|》|是|2007年|华夏出版社|出版|的|图书|，|作者|是|杨文衡</td>\n",
       "      <td>w|nw|w|v|t|nt|v|u|n|w|n|v|nr</td>\n",
       "      <td>中国风水十讲</td>\n",
       "      <td>书籍</td>\n",
       "      <td>华夏出版社</td>\n",
       "      <td>出版社</td>\n",
       "      <td>出版社</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>《中国风水十讲》是2007年华夏出版社出版的图书，作者是杨文衡</td>\n",
       "      <td>《|中国风水十讲|》|是|2007年|华夏出版社|出版|的|图书|，|作者|是|杨文衡</td>\n",
       "      <td>w|nw|w|v|t|nt|v|u|n|w|n|v|nr</td>\n",
       "      <td>中国风水十讲</td>\n",
       "      <td>图书作品</td>\n",
       "      <td>杨文衡</td>\n",
       "      <td>人物</td>\n",
       "      <td>作者</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>《空城未央》是夙言以信创作的网络小说，发表于17K小说网</td>\n",
       "      <td>《|空城未央|》|是|夙言|以|信|创作|的|网络|小说|，|发表|于|17K小说网</td>\n",
       "      <td>w|nw|w|v|n|p|n|v|u|n|n|w|v|p|nz</td>\n",
       "      <td>空城未央</td>\n",
       "      <td>图书作品</td>\n",
       "      <td>夙言以信</td>\n",
       "      <td>人物</td>\n",
       "      <td>作者</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>《空城未央》是夙言以信创作的网络小说，发表于17K小说网</td>\n",
       "      <td>《|空城未央|》|是|夙言|以|信|创作|的|网络|小说|，|发表|于|17K小说网</td>\n",
       "      <td>w|nw|w|v|n|p|n|v|u|n|n|w|v|p|nz</td>\n",
       "      <td>空城未央</td>\n",
       "      <td>网络小说</td>\n",
       "      <td>17K小说网</td>\n",
       "      <td>网站</td>\n",
       "      <td>连载网站</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              text  \\\n",
       "0             内容简介《宜兴紫砂图典》由故宫出版社出版   \n",
       "1  《中国风水十讲》是2007年华夏出版社出版的图书，作者是杨文衡   \n",
       "2  《中国风水十讲》是2007年华夏出版社出版的图书，作者是杨文衡   \n",
       "3     《空城未央》是夙言以信创作的网络小说，发表于17K小说网   \n",
       "4     《空城未央》是夙言以信创作的网络小说，发表于17K小说网   \n",
       "\n",
       "                                     word_segs  \\\n",
       "0                  内容|简介|《|宜兴紫砂图典|》|由|故宫出版社|出版   \n",
       "1  《|中国风水十讲|》|是|2007年|华夏出版社|出版|的|图书|，|作者|是|杨文衡   \n",
       "2  《|中国风水十讲|》|是|2007年|华夏出版社|出版|的|图书|，|作者|是|杨文衡   \n",
       "3   《|空城未央|》|是|夙言|以|信|创作|的|网络|小说|，|发表|于|17K小说网   \n",
       "4   《|空城未央|》|是|夙言|以|信|创作|的|网络|小说|，|发表|于|17K小说网   \n",
       "\n",
       "                          pos_tags subject subject_type  object object_type  \\\n",
       "0                n|n|w|nw|w|p|nt|v  宜兴紫砂图典           书籍   故宫出版社         出版社   \n",
       "1     w|nw|w|v|t|nt|v|u|n|w|n|v|nr  中国风水十讲           书籍   华夏出版社         出版社   \n",
       "2     w|nw|w|v|t|nt|v|u|n|w|n|v|nr  中国风水十讲         图书作品     杨文衡          人物   \n",
       "3  w|nw|w|v|n|p|n|v|u|n|n|w|v|p|nz    空城未央         图书作品    夙言以信          人物   \n",
       "4  w|nw|w|v|n|p|n|v|u|n|n|w|v|p|nz    空城未央         网络小说  17K小说网          网站   \n",
       "\n",
       "  predicate  \n",
       "0       出版社  \n",
       "1       出版社  \n",
       "2        作者  \n",
       "3        作者  \n",
       "4      连载网站  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "影视作品    1486\n",
       "人物      1379\n",
       "歌曲       777\n",
       "图书作品     532\n",
       "书籍       280\n",
       "企业       187\n",
       "网络小说     181\n",
       "生物       163\n",
       "历史人物     106\n",
       "机构        83\n",
       "电视综艺      61\n",
       "行政区       29\n",
       "国家        10\n",
       "景点        10\n",
       "地点         7\n",
       "学科专业       2\n",
       "Name: subject_type, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.subject_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "人物        2771\n",
       "Date       530\n",
       "Text       302\n",
       "地点         292\n",
       "出版社        280\n",
       "国家         208\n",
       "学校         181\n",
       "网站         180\n",
       "目          163\n",
       "音乐专辑       158\n",
       "企业          93\n",
       "Number      91\n",
       "城市          18\n",
       "气候          14\n",
       "作品          10\n",
       "语言           2\n",
       "Name: object_type, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.object_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "def load_data(df, return_vocab=False):\n",
    "    tokens = df.word_segs.apply(lambda x: x.split(\"|\"))\n",
    "    postags = df.pos_tags.apply(lambda x: x.split(\"|\"))\n",
    "    if not return_vocab:\n",
    "        return tokens, postags\n",
    "    token_vocab = list(set(chain.from_iterable(tokens)))\n",
    "    postag_vocab = list(set(chain.from_iterable(postags)))\n",
    "    return tokens, postags, token_vocab, postag_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab: 18213\n",
      "pos vocab: 25\n"
     ]
    }
   ],
   "source": [
    "train_tokens, train_pos_tokens, vocabs, pos_vocabs = load_data(train_data, return_vocab=True)\n",
    "dev_tokens, dev_pos_tokens = load_data(dev_data, return_vocab=False)\n",
    "\n",
    "print(\"vocab:\", len(vocabs))\n",
    "print(\"pos vocab:\", len(pos_vocabs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['影视作品', '人物', '歌曲', '图书作品', '书籍', '企业', '网络小说', '生物', '历史人物', '机构', '电视综艺', '行政区', '国家', '景点', '地点', '学科专业']\n",
      "['人物', 'Date', 'Text', '地点', '出版社', '国家', '学校', '网站', '目', '音乐专辑', '企业', 'Number', '城市', '气候', '作品', '语言']\n"
     ]
    }
   ],
   "source": [
    "pos_tags = pos_vocabs\n",
    "subject_types = train_data.subject_type.value_counts().index.to_list()\n",
    "print(subject_types)\n",
    "object_types = train_data.object_type.value_counts().index.to_list()\n",
    "print(object_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tokens_to_ids(tokens, mapping):\n",
    "    return [mapping.get(i, 0) for i in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_to_id = dict(zip(vocabs, range(1, len(vocabs) + 1)))\n",
    "pos_to_id = dict(zip(pos_tags, range(1, len(pos_tags) + 1)))\n",
    "train_tokens = [convert_tokens_to_ids(i, mapping=token_to_id) for i in train_tokens]\n",
    "dev_tokens = [convert_tokens_to_ids(i, mapping=pos_to_id) for i in dev_tokens]\n",
    "train_pos_tokens = [convert_tokens_to_ids(i, mapping=token_to_id) for i in train_pos_tokens]\n",
    "dev_pos_tokens = [convert_tokens_to_ids(i, mapping=pos_to_id) for i in dev_pos_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOKEN: 0 1.0\n",
      "POS: 0 1.0\n",
      "TOKEN: 25 16.0\n",
      "POS: 25 16.0\n",
      "TOKEN: 50 25.0\n",
      "POS: 50 25.0\n",
      "TOKEN: 75 36.0\n",
      "POS: 75 36.0\n",
      "TOKEN: 90 54.0\n",
      "POS: 90 54.0\n",
      "TOKEN: 95 68.0\n",
      "POS: 95 68.0\n",
      "TOKEN: 99 110.0\n",
      "POS: 99 110.0\n",
      "TOKEN: 100 173.0\n",
      "POS: 100 173.0\n"
     ]
    }
   ],
   "source": [
    "len_of_token = [len(i) for i in train_tokens]\n",
    "len_of_pos = [len(i) for i in train_pos_tokens]\n",
    "for p in [0, 25, 50, 75, 90, 95, 99, 100]:\n",
    "    print(\"TOKEN:\", p, np.percentile(len_of_token, p))\n",
    "    print(\"POS:\", p, np.percentile(len_of_pos, p))\n",
    "maxlen = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = pad_sequences(train_tokens, maxlen=maxlen)\n",
    "dev_tokens = pad_sequences(dev_tokens, maxlen=maxlen)\n",
    "\n",
    "train_pos_tokens = pad_sequences(train_pos_tokens, maxlen=maxlen)\n",
    "dev_pos_tokens = pad_sequences(dev_pos_tokens, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 16\n"
     ]
    }
   ],
   "source": [
    "subject_to_id = dict(zip(subject_types, range(len(subject_types) + 1)))\n",
    "train_subjects = train_data.subject_type.map(subject_to_id).values\n",
    "dev_subjects = dev_data.subject_type.map(subject_to_id).values\n",
    "\n",
    "object_to_id = dict(zip(object_types, range(len(object_types) + 1)))\n",
    "train_objects = train_data.object_type.map(object_to_id).values\n",
    "dev_objects = dev_data.object_type.map(object_to_id).values\n",
    "print(len(subject_to_id), len(object_to_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = train_data.predicate.unique()\n",
    "label_to_id = dict(zip(labels, range(1, len(labels) + 1)))\n",
    "y_train = train_data.predicate.map(label_to_id).values\n",
    "y_dev = dev_data.predicate.map(label_to_id).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Position encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"内容简介《宜兴紫砂图典》由故宫出版社出版\"\n",
    "tokens = \"内容|简介|《|宜兴|紫砂|图典|》|由|故宫出版社|出版\".split(\"|\")\n",
    "subject = \"宜兴紫砂图典\"\n",
    "\n",
    "i = 0\n",
    "positions = []\n",
    "for token in tokens:\n",
    "    if token not in subject:\n",
    "        positions.append(i)\n",
    "        i += 1\n",
    "        continue\n",
    "    \n",
    "text.index(subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, CuDNNLSTM, GlobalMaxPooling1D, Concatenate, Embedding, Dropout, Reshape\n",
    "from keras.layers import GlobalAveragePooling1D\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/heyao/.conda/envs/venv3.7/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "y_train_onehot = onehot_encoder.fit_transform(y_train.reshape(-1, 1))\n",
    "y_dev_onehot = onehot_encoder.transform(y_dev.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(seq_tokens_shape, seq_pos_shape, cat_emb_shapes, lstm_size=40, \n",
    "                num_class=train_data.predicate.nunique()):\n",
    "    \"\"\"without position embedding\n",
    "    train: 0.82099\n",
    "    dev  : 0.75307\n",
    "    \"\"\"\n",
    "    seq_token_input = Input((maxlen, ))\n",
    "    seq_output = Embedding(*seq_tokens_shape)(seq_token_input)\n",
    "    seq_output = CuDNNLSTM(lstm_size, return_sequences=True)(seq_output)\n",
    "    seq_output = GlobalAveragePooling1D()(seq_output)\n",
    "    \n",
    "    seq_pos_input = Input((maxlen, ))\n",
    "    seq_pos_output = Embedding(*seq_pos_shape)(seq_pos_input)\n",
    "    seq_pos_output = CuDNNLSTM(lstm_size, return_sequences=True)(seq_pos_output)\n",
    "    seq_pos_output = GlobalAveragePooling1D()(seq_pos_output)\n",
    "    \n",
    "    inputs = [seq_token_input, seq_pos_input]\n",
    "    outputs = [seq_output, seq_pos_output]\n",
    "    for cat_vocab, cat_dim in cat_emb_shapes:\n",
    "        cat_input = Input((1, ))\n",
    "        cat_output = Embedding(cat_vocab, cat_dim)(cat_input)\n",
    "        cat_output = Reshape((cat_dim, ))(cat_output)\n",
    "        cat_output = Dense(128, activation=\"relu\")(cat_output)\n",
    "        inputs.append(cat_input)\n",
    "        outputs.append(cat_output)\n",
    "    \n",
    "    x = Concatenate()(outputs)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(128, activation=\"relu\")(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(num_class, activation=\"softmax\")(x)\n",
    "    model = Model(inputs, x)\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_model(seq_tokens_shape, seq_pos_shape, cat_emb_shapes, lstm_size=40, \n",
    "                num_class=train_data.predicate.nunique()):\n",
    "    seq_token_input = Input((maxlen, ))\n",
    "    seq_output = Embedding(*seq_tokens_shape)(seq_token_input)\n",
    "    seq_output = CuDNNLSTM(lstm_size, return_sequences=True)(seq_output)\n",
    "    seq_output = GlobalAveragePooling1D()(seq_output)\n",
    "    \n",
    "    seq_pos_input = Input((maxlen, ))\n",
    "    seq_pos_output = Embedding(*seq_pos_shape)(seq_pos_input)\n",
    "    seq_pos_output = CuDNNLSTM(lstm_size, return_sequences=True)(seq_pos_output)\n",
    "    seq_pos_output = GlobalAveragePooling1D()(seq_pos_output)\n",
    "    \n",
    "    inputs = [seq_token_input, seq_pos_input]\n",
    "    outputs = [seq_output, seq_pos_output]\n",
    "    for cat_vocab, cat_dim in cat_emb_shapes:\n",
    "        cat_input = Input((1, ))\n",
    "        cat_output = Embedding(cat_vocab, cat_dim)(cat_input)\n",
    "        cat_output = Reshape((cat_dim, ))(cat_output)\n",
    "        cat_output = Dense(128, activation=\"relu\")(cat_output)\n",
    "        inputs.append(cat_input)\n",
    "        outputs.append(cat_output)\n",
    "    \n",
    "    x = Concatenate()(outputs)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(128, activation=\"relu\")(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(num_class, activation=\"softmax\")(x)\n",
    "    model = Model(inputs, x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5293 samples, validate on 2073 samples\n",
      "Epoch 1/40\n",
      "5293/5293 [==============================] - 7s 1ms/step - loss: 3.1934 - val_loss: 2.3461\n",
      "Epoch 2/40\n",
      "5293/5293 [==============================] - 1s 265us/step - loss: 1.7661 - val_loss: 1.1188\n",
      "Epoch 3/40\n",
      "5293/5293 [==============================] - 1s 266us/step - loss: 1.0792 - val_loss: 0.7274\n",
      "Epoch 4/40\n",
      "5293/5293 [==============================] - 1s 267us/step - loss: 0.7896 - val_loss: 0.5710\n",
      "Epoch 5/40\n",
      "5293/5293 [==============================] - 1s 265us/step - loss: 0.6513 - val_loss: 0.5270\n",
      "Epoch 6/40\n",
      "5293/5293 [==============================] - 1s 269us/step - loss: 0.5863 - val_loss: 0.5207\n",
      "Epoch 7/40\n",
      "5293/5293 [==============================] - 1s 271us/step - loss: 0.5415 - val_loss: 0.4857\n",
      "Epoch 8/40\n",
      "5293/5293 [==============================] - 1s 267us/step - loss: 0.4946 - val_loss: 0.4845\n",
      "Epoch 9/40\n",
      "5293/5293 [==============================] - 1s 268us/step - loss: 0.4717 - val_loss: 0.4745\n",
      "Epoch 10/40\n",
      "5293/5293 [==============================] - 1s 264us/step - loss: 0.4573 - val_loss: 0.4867\n",
      "Epoch 11/40\n",
      "5293/5293 [==============================] - 1s 264us/step - loss: 0.4361 - val_loss: 0.4752\n",
      "Epoch 12/40\n",
      "5293/5293 [==============================] - 1s 265us/step - loss: 0.4163 - val_loss: 0.4967\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4e0874b6d8>"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = build_model((len(vocabs) + 1, 100), (len(pos_tags) + 1, 8), [(16 + 1, 4), (16 + 1, 4)], lstm_size=20)\n",
    "model.compile(optimizers.Adam(), \"categorical_crossentropy\")\n",
    "callback_list = [\n",
    "    callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n",
    "]\n",
    "model.fit([train_tokens, train_pos_tokens, train_subjects, train_objects], y_train_onehot, \n",
    "          validation_data=([dev_tokens, dev_pos_tokens, dev_subjects, dev_objects], y_dev_onehot), epochs=40, \n",
    "          batch_size=64, verbose=1, callbacks=callback_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = model.predict([train_tokens, train_pos_tokens, train_subjects, train_objects], batch_size=512)\n",
    "dev_pred = model.predict([dev_tokens, dev_pos_tokens, dev_subjects, dev_objects], batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8260985863104171\n",
      "0.7470346404439404\n"
     ]
    }
   ],
   "source": [
    "print(metrics.f1_score(y_train, train_pred.argmax(axis=1) + 1, average=\"weighted\"))\n",
    "print(metrics.f1_score(y_dev, dev_pred.argmax(axis=1) + 1, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(metrics.classification_report(y_train, train_pred.argmax(axis=1) + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(metrics.classification_report(y_dev, dev_pred.argmax(axis=1) + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|              Title              |      Train     |    Dev     |\n",
    "|               ---               |       ---      |    ---     |\n",
    "|    without postion embedding    |     0.82099    |  0.75307   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
